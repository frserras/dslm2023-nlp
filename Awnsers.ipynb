{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises\n",
        "## Exercise 1 - Using the Hugging Face pipeline, apply the Named Entity Recognition (ner) model *dslim/bert-base-NER* to the sentence \"My name is [Your Name Here] and I live in [Your City Here]\""
      ],
      "metadata": {
        "id": "yoSPCp_FHSbl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVHT83XHGyKv",
        "outputId": "be738c3b-5f3d-4e18-f375-26bc97e48505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity': 'B-PER', 'score': 0.99655044, 'index': 4, 'word': 'Felipe', 'start': 11, 'end': 17}, {'entity': 'B-LOC', 'score': 0.9992907, 'index': 9, 'word': 'São', 'start': 32, 'end': 35}, {'entity': 'I-LOC', 'score': 0.9993781, 'index': 10, 'word': 'Paulo', 'start': 36, 'end': 41}]\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers tokenizers datasets\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "example = \"My name is Felipe and I live in São Paulo\"\n",
        "\n",
        "ner_results = nlp(example)\n",
        "print(ner_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "In this exercise, we will briefly explore the metrics typically employed to evaluate classification models. These metrics are commonly applied to evaluate BERT when used for text classification.\n",
        "\n",
        "Please fill in the missing content in the following functions to calculate accuracy, precision, and recall metrics. You can refer to the definitions of these metrics on Wikipedia. Below, we present a function to test your metrics and verify the success of your implementation."
      ],
      "metadata": {
        "id": "D9ykyhMMi2ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score as accuracy\n",
        "from sklearn.metrics import precision_score as precision\n",
        "from sklearn.metrics import recall_score as recall\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def test_metric(user_metric, reference_metric, num_tests=100, samples_per_test=100, seed=12345, tolerance=1e-4):\n",
        "    for i in tqdm(range(num_tests)):\n",
        "        y_pred = list(np.random.randint(2, size=samples_per_test))\n",
        "        y_gold = list(np.random.randint(2, size=samples_per_test))\n",
        "        user_metric_value = user_metric(y_gold, y_pred)\n",
        "        reference_metric_value = reference_metric(y_gold, y_pred)\n",
        "        if abs(user_metric_value - reference_metric_value) > tolerance:\n",
        "            raise ValueError(f\"Test Failed. user_metric returned {user_metric_value}, but {reference_metric_value} was expected.\")\n",
        "    print(\"\\nThe function passed all tests\")"
      ],
      "metadata": {
        "id": "bkfPcZIwjLsB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_accuracy(y_gold, y_pred):\n",
        "    if len(y_pred) != len(y_gold):\n",
        "        raise ValueError(\"Input lists must have the same length.\")\n",
        "\n",
        "    true_positives = sum(1 for pred, gold in zip(y_pred, y_gold) if pred == 1.0 and gold == 1.0)\n",
        "    true_negatives = sum(1 for pred, gold in zip(y_pred, y_gold) if pred == 0.0 and gold == 0.0)\n",
        "    false_positives = sum(1 for pred, gold in zip(y_pred, y_gold) if pred == 1.0 and gold == 0.0)\n",
        "    false_negatives = sum(1 for pred, gold in zip(y_pred, y_gold) if pred == 0.0 and gold == 1.0)\n",
        "\n",
        "    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "4v_gu-fpi4tZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metric(my_accuracy, accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKbuVQH-jYV0",
        "outputId": "b84a9575-ea75-479d-f1bb-aef1607f8a0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 329.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The function passed all tests\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_precision(y_gold, y_pred):\n",
        "\n",
        "    if len(y_pred) != len(y_gold):\n",
        "        raise ValueError(\"Input lists must have the same length.\")\n",
        "\n",
        "    true_positives = sum(1 for pred, gold in zip(y_pred, y_gold) if pred == 1.0 and gold == 1.0)\n",
        "    false_positives = sum(1 for pred, gold in zip(y_pred, y_gold) if pred == 1.0 and gold == 0.0)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if true_positives + false_positives == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives)\n",
        "\n",
        "\n",
        "    return precision\n"
      ],
      "metadata": {
        "id": "dhdXK21BjAOt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metric(my_precision, precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQlPcIX6jgPh",
        "outputId": "955096ed-8529-41db-bad7-f4797a126ea6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 503.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The function passed all tests\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def my_recall(y_gold, y_pred):\n",
        "    if len(y_pred) != len(y_gold):\n",
        "        raise ValueError(\"Input lists must have the same length.\")\n",
        "    true_positive = sum(1 for gold, pred in zip(y_gold, y_pred) if gold == 1 and pred == 1)\n",
        "    actual_positive = sum(1 for gold in y_gold if gold == 1)\n",
        "    if actual_positive == 0:\n",
        "        recall=0\n",
        "    else:\n",
        "        recall = true_positive / actual_positive\n",
        "        return recall"
      ],
      "metadata": {
        "id": "0E4XKG5gjBKj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metric(my_recall, recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jr361PBjLIL",
        "outputId": "14472010-b827-4535-e84f-2a0c0bc3517a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 252.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The function passed all tests\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}