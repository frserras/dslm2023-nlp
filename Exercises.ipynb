{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoSPCp_FHSbl"
      },
      "source": [
        "# Exercises\n",
        "## Exercise 1 - Using the Hugging Face pipeline, apply the Named Entity Recognition (ner) model *dslim/bert-base-NER* to the sentence \"My name is [Your Name Here] and I live in [Your City Here]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVHT83XHGyKv",
        "outputId": "be738c3b-5f3d-4e18-f375-26bc97e48505"
      },
      "outputs": [],
      "source": [
        "# [Your solution here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9ykyhMMi2ZZ"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "In this exercise, we will briefly explore the metrics typically employed to evaluate classification models. These metrics are commonly applied to evaluate BERT when used for text classification.\n",
        "\n",
        "Please fill in the missing content in the following functions to calculate accuracy, precision, and recall metrics. You can refer to the definitions of these metrics on Wikipedia. Below, we present a function to test your metrics and verify the success of your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkfPcZIwjLsB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score as accuracy\n",
        "from sklearn.metrics import precision_score as precision\n",
        "from sklearn.metrics import recall_score as recall\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def test_metric(user_metric, reference_metric, num_tests=100, samples_per_test=100, seed=12345, tolerance=1e-4):\n",
        "    for i in tqdm(range(num_tests)):\n",
        "        y_pred = list(np.random.randint(2, size=samples_per_test))\n",
        "        y_gold = list(np.random.randint(2, size=samples_per_test))\n",
        "        user_metric_value = user_metric(y_gold, y_pred)\n",
        "        reference_metric_value = reference_metric(y_gold, y_pred)\n",
        "        if abs(user_metric_value - reference_metric_value) > tolerance:\n",
        "            raise ValueError(f\"Test Failed. user_metric returned {user_metric_value}, but {reference_metric_value} was expected.\")\n",
        "    print(\"\\nThe function passed all tests\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4v_gu-fpi4tZ"
      },
      "outputs": [],
      "source": [
        "def my_accuracy(y_gold, y_pred):\n",
        "   \n",
        "    # [Your solution here]\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKbuVQH-jYV0",
        "outputId": "b84a9575-ea75-479d-f1bb-aef1607f8a0b"
      },
      "outputs": [],
      "source": [
        "test_metric(my_accuracy, accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhdXK21BjAOt"
      },
      "outputs": [],
      "source": [
        "def my_precision(y_gold, y_pred):\n",
        "    \n",
        "    # [Your solution here]\n",
        "\n",
        "    return precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQlPcIX6jgPh",
        "outputId": "955096ed-8529-41db-bad7-f4797a126ea6"
      },
      "outputs": [],
      "source": [
        "test_metric(my_precision, precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E4XKG5gjBKj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def my_recall(y_gold, y_pred):\n",
        "    # [Your solution here]\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jr361PBjLIL",
        "outputId": "14472010-b827-4535-e84f-2a0c0bc3517a"
      },
      "outputs": [],
      "source": [
        "test_metric(my_recall, recall)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
